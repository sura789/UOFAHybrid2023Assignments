{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "yrAk1SLm3kvL"
      },
      "source": [
        "# Sequence Labeling\n",
        "\n",
        "In this assignment, you will work on the [MeasEval](https://competitions.codalab.org/competitions/25770) shared task that was part of SemEval-2021. The goal of **MeasEval** is  the extraction of counts, measurements, and related context from scientific documents. The task is a complex problem that involves solving a number of steps that range from identifying quantities and units of measurement to identify relationships between them. For this assignment, you will focus only on the *Quantity* recognition step:\n",
        "\n",
        "*  Given a paragraph from a scientific text, identify all spans containing quantities like *12 kg*. This problem can be approached as a Sequence Labeling task.\n",
        "\n",
        "You will develop a Recurrent Neural Network with [Keras](https://keras.io/), a high-level Deep Learning API written in **Python** that provides a user-friendly interface for the [TensorFlow](https://www.tensorflow.org/) library, one of the most popular low-level Deep Learning frameworks. You will use the following objects and functions:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Mount Drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_KTa2YayqWLk",
        "outputId": "1d0c35fc-cc79-4a38-f8bb-015a8fb86c1c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r /content/drive/MyDrive/AIML/NLP_requirements_Assign3.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Epy7f1b6qW3L",
        "outputId": "fa1c4d41-9c8b-4926-b8ed-0bf391363d07"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ipython==8.5.0 (from -r /content/drive/MyDrive/AIML/NLP_requirements_Assign3.txt (line 1))\n",
            "  Downloading ipython-8.5.0-py3-none-any.whl (752 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m752.0/752.0 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jupyter==1.0.0 (from -r /content/drive/MyDrive/AIML/NLP_requirements_Assign3.txt (line 2))\n",
            "  Downloading jupyter-1.0.0-py2.py3-none-any.whl (2.7 kB)\n",
            "Collecting nbimporter==0.3.4 (from -r /content/drive/MyDrive/AIML/NLP_requirements_Assign3.txt (line 3))\n",
            "  Downloading nbimporter-0.3.4-py3-none-any.whl (4.9 kB)\n",
            "Collecting pytest==7.1.3 (from -r /content/drive/MyDrive/AIML/NLP_requirements_Assign3.txt (line 4))\n",
            "  Downloading pytest-7.1.3-py3-none-any.whl (298 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.2/298.2 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pandas==1.3.5 (from -r /content/drive/MyDrive/AIML/NLP_requirements_Assign3.txt (line 5))\n",
            "  Downloading pandas-1.3.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.5/11.5 MB\u001b[0m \u001b[31m40.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting spacy==3.2.4 (from -r /content/drive/MyDrive/AIML/NLP_requirements_Assign3.txt (line 6))\n",
            "  Downloading spacy-3.2.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.1/6.1 MB\u001b[0m \u001b[31m56.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow==2.9.2 (from -r /content/drive/MyDrive/AIML/NLP_requirements_Assign3.txt (line 7))\n",
            "  Downloading tensorflow-2.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (511.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m511.8/511.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting numpy==1.24.2 (from -r /content/drive/MyDrive/AIML/NLP_requirements_Assign3.txt (line 8))\n",
            "  Downloading numpy-1.24.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m46.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scikit-learn==1.2.0 (from -r /content/drive/MyDrive/AIML/NLP_requirements_Assign3.txt (line 9))\n",
            "  Downloading scikit_learn-1.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m67.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython==8.5.0->-r /content/drive/MyDrive/AIML/NLP_requirements_Assign3.txt (line 1)) (0.2.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython==8.5.0->-r /content/drive/MyDrive/AIML/NLP_requirements_Assign3.txt (line 1)) (4.4.2)\n",
            "Collecting jedi>=0.16 (from ipython==8.5.0->-r /content/drive/MyDrive/AIML/NLP_requirements_Assign3.txt (line 1))\n",
            "  Downloading jedi-0.19.0-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m49.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython==8.5.0->-r /content/drive/MyDrive/AIML/NLP_requirements_Assign3.txt (line 1)) (0.1.6)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython==8.5.0->-r /content/drive/MyDrive/AIML/NLP_requirements_Assign3.txt (line 1)) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit<3.1.0,>3.0.1 in /usr/local/lib/python3.10/dist-packages (from ipython==8.5.0->-r /content/drive/MyDrive/AIML/NLP_requirements_Assign3.txt (line 1)) (3.0.39)\n",
            "Requirement already satisfied: pygments>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from ipython==8.5.0->-r /content/drive/MyDrive/AIML/NLP_requirements_Assign3.txt (line 1)) (2.16.1)\n",
            "Collecting stack-data (from ipython==8.5.0->-r /content/drive/MyDrive/AIML/NLP_requirements_Assign3.txt (line 1))\n",
            "  Downloading stack_data-0.6.2-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: traitlets>=5 in /usr/local/lib/python3.10/dist-packages (from ipython==8.5.0->-r /content/drive/MyDrive/AIML/NLP_requirements_Assign3.txt (line 1)) (5.7.1)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython==8.5.0->-r /content/drive/MyDrive/AIML/NLP_requirements_Assign3.txt (line 1)) (4.8.0)\n",
            "Requirement already satisfied: notebook in /usr/local/lib/python3.10/dist-packages (from jupyter==1.0.0->-r /content/drive/MyDrive/AIML/NLP_requirements_Assign3.txt (line 2)) (6.5.5)\n",
            "Collecting qtconsole (from jupyter==1.0.0->-r /content/drive/MyDrive/AIML/NLP_requirements_Assign3.txt (line 2))\n",
            "  Downloading qtconsole-5.4.4-py3-none-any.whl (121 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.9/121.9 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jupyter-console in /usr/local/lib/python3.10/dist-packages (from jupyter==1.0.0->-r /content/drive/MyDrive/AIML/NLP_requirements_Assign3.txt (line 2)) (6.1.0)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.10/dist-packages (from jupyter==1.0.0->-r /content/drive/MyDrive/AIML/NLP_requirements_Assign3.txt (line 2)) (6.5.4)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.10/dist-packages (from jupyter==1.0.0->-r /content/drive/MyDrive/AIML/NLP_requirements_Assign3.txt (line 2)) (5.5.6)\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.10/dist-packages (from jupyter==1.0.0->-r /content/drive/MyDrive/AIML/NLP_requirements_Assign3.txt (line 2)) (7.7.1)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.10/dist-packages (from pytest==7.1.3->-r /content/drive/MyDrive/AIML/NLP_requirements_Assign3.txt (line 4)) (23.1.0)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.10/dist-packages (from pytest==7.1.3->-r /content/drive/MyDrive/AIML/NLP_requirements_Assign3.txt (line 4)) (2.0.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from pytest==7.1.3->-r /content/drive/MyDrive/AIML/NLP_requirements_Assign3.txt (line 4)) (23.1)\n",
            "Requirement already satisfied: pluggy<2.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from pytest==7.1.3->-r /content/drive/MyDrive/AIML/NLP_requirements_Assign3.txt (line 4)) (1.3.0)\n",
            "Collecting py>=1.8.2 (from pytest==7.1.3->-r /content/drive/MyDrive/AIML/NLP_requirements_Assign3.txt (line 4))\n",
            "  Downloading py-1.11.0-py2.py3-none-any.whl (98 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.7/98.7 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tomli>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pytest==7.1.3->-r /content/drive/MyDrive/AIML/NLP_requirements_Assign3.txt (line 4)) (2.0.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.10/dist-packages (from pandas==1.3.5->-r /content/drive/MyDrive/AIML/NLP_requirements_Assign3.txt (line 5)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.10/dist-packages (from pandas==1.3.5->-r /content/drive/MyDrive/AIML/NLP_requirements_Assign3.txt (line 5)) (2023.3.post1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /usr/local/lib/python3.10/dist-packages (from spacy==3.2.4->-r /content/drive/MyDrive/AIML/NLP_requirements_Assign3.txt (line 6)) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy==3.2.4->-r /content/drive/MyDrive/AIML/NLP_requirements_Assign3.txt (line 6)) (1.0.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy==3.2.4->-r /content/drive/MyDrive/AIML/NLP_requirements_Assign3.txt (line 6)) (1.0.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy==3.2.4->-r /content/drive/MyDrive/AIML/NLP_requirements_Assign3.txt (line 6)) (2.0.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy==3.2.4->-r /content/drive/MyDrive/AIML/NLP_requirements_Assign3.txt (line 6)) (3.0.8)\n",
            "Collecting thinc<8.1.0,>=8.0.12 (from spacy==3.2.4->-r /content/drive/MyDrive/AIML/NLP_requirements_Assign3.txt (line 6))\n",
            "  Downloading thinc-8.0.17-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (659 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m659.5/659.5 kB\u001b[0m \u001b[31m31.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from spacy==3.2.4->-r /content/drive/MyDrive/AIML/NLP_requirements_Assign3.txt (line 6)) (0.7.10)\n",
            "Collecting wasabi<1.1.0,>=0.8.1 (from spacy==3.2.4->-r /content/drive/MyDrive/AIML/NLP_requirements_Assign3.txt (line 6))\n",
            "  Downloading wasabi-0.10.1-py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from spacy==3.2.4->-r /content/drive/MyDrive/AIML/NLP_requirements_Assign3.txt (line 6)) (2.4.7)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy==3.2.4->-r /content/drive/MyDrive/AIML/NLP_requirements_Assign3.txt (line 6)) (2.0.9)\n",
            "Collecting typer<0.5.0,>=0.3.0 (from spacy==3.2.4->-r /content/drive/MyDrive/AIML/NLP_requirements_Assign3.txt (line 6))\n",
            "  Downloading typer-0.4.2-py3-none-any.whl (27 kB)\n",
            "Collecting click<8.1.0 (from spacy==3.2.4->-r /content/drive/MyDrive/AIML/NLP_requirements_Assign3.txt (line 6))\n",
            "  Downloading click-8.0.4-py3-none-any.whl (97 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.5/97.5 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.10/dist-packages (from spacy==3.2.4->-r /content/drive/MyDrive/AIML/NLP_requirements_Assign3.txt (line 6)) (0.10.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy==3.2.4->-r /content/drive/MyDrive/AIML/NLP_requirements_Assign3.txt (line 6)) (4.66.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy==3.2.4->-r /content/drive/MyDrive/AIML/NLP_requirements_Assign3.txt (line 6)) (2.31.0)\n",
            "Collecting pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 (from spacy==3.2.4->-r /content/drive/MyDrive/AIML/NLP_requirements_Assign3.txt (line 6))\n",
            "  Downloading pydantic-1.8.2-py3-none-any.whl (126 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.0/126.0 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy==3.2.4->-r /content/drive/MyDrive/AIML/NLP_requirements_Assign3.txt (line 6)) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy==3.2.4->-r /content/drive/MyDrive/AIML/NLP_requirements_Assign3.txt (line 6)) (67.7.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy==3.2.4->-r /content/drive/MyDrive/AIML/NLP_requirements_Assign3.txt (line 6)) (3.3.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.2->-r /content/drive/MyDrive/AIML/NLP_requirements_Assign3.txt (line 7)) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.2->-r /content/drive/MyDrive/AIML/NLP_requirements_Assign3.txt (line 7)) (1.6.3)\n",
            "Collecting flatbuffers<2,>=1.12 (from tensorflow==2.9.2->-r /content/drive/MyDrive/AIML/NLP_requirements_Assign3.txt (line 7))\n",
            "  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.2->-r /content/drive/MyDrive/AIML/NLP_requirements_Assign3.txt (line 7)) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.2->-r /content/drive/MyDrive/AIML/NLP_requirements_Assign3.txt (line 7)) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.2->-r /content/drive/MyDrive/AIML/NLP_requirements_Assign3.txt (line 7)) (1.57.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.2->-r /content/drive/MyDrive/AIML/NLP_requirements_Assign3.txt (line 7)) (3.9.0)\n",
            "Collecting keras<2.10.0,>=2.9.0rc0 (from tensorflow==2.9.2->-r /content/drive/MyDrive/AIML/NLP_requirements_Assign3.txt (line 7))\n",
            "  Downloading keras-2.9.0-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m50.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting keras-preprocessing>=1.1.1 (from tensorflow==2.9.2->-r /content/drive/MyDrive/AIML/NLP_requirements_Assign3.txt (line 7))\n",
            "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.2->-r /content/drive/MyDrive/AIML/NLP_requirements_Assign3.txt (line 7)) (16.0.6)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.2->-r /content/drive/MyDrive/AIML/NLP_requirements_Assign3.txt (line 7)) (3.3.0)\n",
            "Collecting protobuf<3.20,>=3.9.2 (from tensorflow==2.9.2->-r /content/drive/MyDrive/AIML/NLP_requirements_Assign3.txt (line 7))\n",
            "  Downloading protobuf-3.19.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m55.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.2->-r /content/drive/MyDrive/AIML/NLP_requirements_Assign3.txt (line 7)) (1.16.0)\n",
            "Collecting tensorboard<2.10,>=2.9 (from tensorflow==2.9.2->-r /content/drive/MyDrive/AIML/NLP_requirements_Assign3.txt (line 7))\n",
            "  Downloading tensorboard-2.9.1-py3-none-any.whl (5.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m91.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.2->-r /content/drive/MyDrive/AIML/NLP_requirements_Assign3.txt (line 7)) (0.33.0)\n",
            "Collecting tensorflow-estimator<2.10.0,>=2.9.0rc0 (from tensorflow==2.9.2->-r /content/drive/MyDrive/AIML/NLP_requirements_Assign3.txt (line 7))\n",
            "  Downloading tensorflow_estimator-2.9.0-py2.py3-none-any.whl (438 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m438.7/438.7 kB\u001b[0m \u001b[31m33.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.2->-r /content/drive/MyDrive/AIML/NLP_requirements_Assign3.txt (line 7)) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.2->-r /content/drive/MyDrive/AIML/NLP_requirements_Assign3.txt (line 7)) (4.5.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.2->-r /content/drive/MyDrive/AIML/NLP_requirements_Assign3.txt (line 7)) (1.15.0)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.2.0->-r /content/drive/MyDrive/AIML/NLP_requirements_Assign3.txt (line 9)) (1.11.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.2.0->-r /content/drive/MyDrive/AIML/NLP_requirements_Assign3.txt (line 9)) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.2.0->-r /content/drive/MyDrive/AIML/NLP_requirements_Assign3.txt (line 9)) (3.2.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.9.2->-r /content/drive/MyDrive/AIML/NLP_requirements_Assign3.txt (line 7)) (0.41.2)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython==8.5.0->-r /content/drive/MyDrive/AIML/NLP_requirements_Assign3.txt (line 1)) (0.8.3)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from pathy>=0.3.5->spacy==3.2.4->-r /content/drive/MyDrive/AIML/NLP_requirements_Assign3.txt (line 6)) (6.4.0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython==8.5.0->-r /content/drive/MyDrive/AIML/NLP_requirements_Assign3.txt (line 1)) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit<3.1.0,>3.0.1->ipython==8.5.0->-r /content/drive/MyDrive/AIML/NLP_requirements_Assign3.txt (line 1)) (0.2.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.2.4->-r /content/drive/MyDrive/AIML/NLP_requirements_Assign3.txt (line 6)) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.2.4->-r /content/drive/MyDrive/AIML/NLP_requirements_Assign3.txt (line 6)) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.2.4->-r /content/drive/MyDrive/AIML/NLP_requirements_Assign3.txt (line 6)) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.2.4->-r /content/drive/MyDrive/AIML/NLP_requirements_Assign3.txt (line 6)) (2023.7.22)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.2->-r /content/drive/MyDrive/AIML/NLP_requirements_Assign3.txt (line 7)) (2.17.3)\n",
            "Collecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard<2.10,>=2.9->tensorflow==2.9.2->-r /content/drive/MyDrive/AIML/NLP_requirements_Assign3.txt (line 7))\n",
            "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.2->-r /content/drive/MyDrive/AIML/NLP_requirements_Assign3.txt (line 7)) (3.4.4)\n",
            "Collecting tensorboard-data-server<0.7.0,>=0.6.0 (from tensorboard<2.10,>=2.9->tensorflow==2.9.2->-r /content/drive/MyDrive/AIML/NLP_requirements_Assign3.txt (line 7))\n",
            "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m75.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorboard-plugin-wit>=1.6.0 (from tensorboard<2.10,>=2.9->tensorflow==2.9.2->-r /content/drive/MyDrive/AIML/NLP_requirements_Assign3.txt (line 7))\n",
            "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.3/781.3 kB\u001b[0m \u001b[31m54.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.2->-r /content/drive/MyDrive/AIML/NLP_requirements_Assign3.txt (line 7)) (2.3.7)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.10/dist-packages (from ipykernel->jupyter==1.0.0->-r /content/drive/MyDrive/AIML/NLP_requirements_Assign3.txt (line 2)) (0.2.0)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.10/dist-packages (from ipykernel->jupyter==1.0.0->-r /content/drive/MyDrive/AIML/NLP_requirements_Assign3.txt (line 2)) (6.1.12)\n",
            "Requirement already satisfied: tornado>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipykernel->jupyter==1.0.0->-r /content/drive/MyDrive/AIML/NLP_requirements_Assign3.txt (line 2)) (6.3.2)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets->jupyter==1.0.0->-r /content/drive/MyDrive/AIML/NLP_requirements_Assign3.txt (line 2)) (3.6.5)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets->jupyter==1.0.0->-r /content/drive/MyDrive/AIML/NLP_requirements_Assign3.txt (line 2)) (3.0.8)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy==3.2.4->-r /content/drive/MyDrive/AIML/NLP_requirements_Assign3.txt (line 6)) (2.1.3)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter==1.0.0->-r /content/drive/MyDrive/AIML/NLP_requirements_Assign3.txt (line 2)) (4.9.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter==1.0.0->-r /content/drive/MyDrive/AIML/NLP_requirements_Assign3.txt (line 2)) (4.11.2)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter==1.0.0->-r /content/drive/MyDrive/AIML/NLP_requirements_Assign3.txt (line 2)) (6.0.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter==1.0.0->-r /content/drive/MyDrive/AIML/NLP_requirements_Assign3.txt (line 2)) (0.7.1)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter==1.0.0->-r /content/drive/MyDrive/AIML/NLP_requirements_Assign3.txt (line 2)) (0.4)\n",
            "Requirement already satisfied: jupyter-core>=4.7 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter==1.0.0->-r /content/drive/MyDrive/AIML/NLP_requirements_Assign3.txt (line 2)) (5.3.1)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter==1.0.0->-r /content/drive/MyDrive/AIML/NLP_requirements_Assign3.txt (line 2)) (0.2.2)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter==1.0.0->-r /content/drive/MyDrive/AIML/NLP_requirements_Assign3.txt (line 2)) (0.8.4)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter==1.0.0->-r /content/drive/MyDrive/AIML/NLP_requirements_Assign3.txt (line 2)) (0.8.0)\n",
            "Requirement already satisfied: nbformat>=5.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter==1.0.0->-r /content/drive/MyDrive/AIML/NLP_requirements_Assign3.txt (line 2)) (5.9.2)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter==1.0.0->-r /content/drive/MyDrive/AIML/NLP_requirements_Assign3.txt (line 2)) (1.5.0)\n",
            "Requirement already satisfied: tinycss2 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter==1.0.0->-r /content/drive/MyDrive/AIML/NLP_requirements_Assign3.txt (line 2)) (1.2.1)\n",
            "Requirement already satisfied: pyzmq<25,>=17 in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter==1.0.0->-r /content/drive/MyDrive/AIML/NLP_requirements_Assign3.txt (line 2)) (23.2.1)\n",
            "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter==1.0.0->-r /content/drive/MyDrive/AIML/NLP_requirements_Assign3.txt (line 2)) (23.1.0)\n",
            "Requirement already satisfied: nest-asyncio>=1.5 in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter==1.0.0->-r /content/drive/MyDrive/AIML/NLP_requirements_Assign3.txt (line 2)) (1.5.7)\n",
            "Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter==1.0.0->-r /content/drive/MyDrive/AIML/NLP_requirements_Assign3.txt (line 2)) (1.8.2)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter==1.0.0->-r /content/drive/MyDrive/AIML/NLP_requirements_Assign3.txt (line 2)) (0.17.1)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter==1.0.0->-r /content/drive/MyDrive/AIML/NLP_requirements_Assign3.txt (line 2)) (0.17.1)\n",
            "Requirement already satisfied: nbclassic>=0.4.7 in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter==1.0.0->-r /content/drive/MyDrive/AIML/NLP_requirements_Assign3.txt (line 2)) (1.0.0)\n",
            "Collecting qtpy>=2.4.0 (from qtconsole->jupyter==1.0.0->-r /content/drive/MyDrive/AIML/NLP_requirements_Assign3.txt (line 2))\n",
            "  Downloading QtPy-2.4.0-py3-none-any.whl (93 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.4/93.4 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting executing>=1.2.0 (from stack-data->ipython==8.5.0->-r /content/drive/MyDrive/AIML/NLP_requirements_Assign3.txt (line 1))\n",
            "  Downloading executing-1.2.0-py2.py3-none-any.whl (24 kB)\n",
            "Collecting asttokens>=2.1.0 (from stack-data->ipython==8.5.0->-r /content/drive/MyDrive/AIML/NLP_requirements_Assign3.txt (line 1))\n",
            "  Downloading asttokens-2.4.0-py2.py3-none-any.whl (27 kB)\n",
            "Collecting pure-eval (from stack-data->ipython==8.5.0->-r /content/drive/MyDrive/AIML/NLP_requirements_Assign3.txt (line 1))\n",
            "  Downloading pure_eval-0.2.2-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9.2->-r /content/drive/MyDrive/AIML/NLP_requirements_Assign3.txt (line 7)) (5.3.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9.2->-r /content/drive/MyDrive/AIML/NLP_requirements_Assign3.txt (line 7)) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9.2->-r /content/drive/MyDrive/AIML/NLP_requirements_Assign3.txt (line 7)) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow==2.9.2->-r /content/drive/MyDrive/AIML/NLP_requirements_Assign3.txt (line 7)) (1.3.1)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core>=4.7->nbconvert->jupyter==1.0.0->-r /content/drive/MyDrive/AIML/NLP_requirements_Assign3.txt (line 2)) (3.10.0)\n",
            "Requirement already satisfied: jupyter-server>=1.8 in /usr/local/lib/python3.10/dist-packages (from nbclassic>=0.4.7->notebook->jupyter==1.0.0->-r /content/drive/MyDrive/AIML/NLP_requirements_Assign3.txt (line 2)) (1.24.0)\n",
            "Requirement already satisfied: notebook-shim>=0.2.3 in /usr/local/lib/python3.10/dist-packages (from nbclassic>=0.4.7->notebook->jupyter==1.0.0->-r /content/drive/MyDrive/AIML/NLP_requirements_Assign3.txt (line 2)) (0.2.3)\n",
            "Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.10/dist-packages (from nbformat>=5.1->nbconvert->jupyter==1.0.0->-r /content/drive/MyDrive/AIML/NLP_requirements_Assign3.txt (line 2)) (2.18.0)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.10/dist-packages (from nbformat>=5.1->nbconvert->jupyter==1.0.0->-r /content/drive/MyDrive/AIML/NLP_requirements_Assign3.txt (line 2)) (4.19.0)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.10/dist-packages (from argon2-cffi->notebook->jupyter==1.0.0->-r /content/drive/MyDrive/AIML/NLP_requirements_Assign3.txt (line 2)) (21.2.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->nbconvert->jupyter==1.0.0->-r /content/drive/MyDrive/AIML/NLP_requirements_Assign3.txt (line 2)) (2.5)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->nbconvert->jupyter==1.0.0->-r /content/drive/MyDrive/AIML/NLP_requirements_Assign3.txt (line 2)) (0.5.1)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.1->nbconvert->jupyter==1.0.0->-r /content/drive/MyDrive/AIML/NLP_requirements_Assign3.txt (line 2)) (2023.7.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.1->nbconvert->jupyter==1.0.0->-r /content/drive/MyDrive/AIML/NLP_requirements_Assign3.txt (line 2)) (0.30.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.1->nbconvert->jupyter==1.0.0->-r /content/drive/MyDrive/AIML/NLP_requirements_Assign3.txt (line 2)) (0.10.2)\n",
            "Requirement already satisfied: anyio<4,>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook->jupyter==1.0.0->-r /content/drive/MyDrive/AIML/NLP_requirements_Assign3.txt (line 2)) (3.7.1)\n",
            "Requirement already satisfied: websocket-client in /usr/local/lib/python3.10/dist-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook->jupyter==1.0.0->-r /content/drive/MyDrive/AIML/NLP_requirements_Assign3.txt (line 2)) (1.6.2)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9.2->-r /content/drive/MyDrive/AIML/NLP_requirements_Assign3.txt (line 7)) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow==2.9.2->-r /content/drive/MyDrive/AIML/NLP_requirements_Assign3.txt (line 7)) (3.2.2)\n",
            "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from argon2-cffi-bindings->argon2-cffi->notebook->jupyter==1.0.0->-r /content/drive/MyDrive/AIML/NLP_requirements_Assign3.txt (line 2)) (1.15.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook->jupyter==1.0.0->-r /content/drive/MyDrive/AIML/NLP_requirements_Assign3.txt (line 2)) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook->jupyter==1.0.0->-r /content/drive/MyDrive/AIML/NLP_requirements_Assign3.txt (line 2)) (1.1.3)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook->jupyter==1.0.0->-r /content/drive/MyDrive/AIML/NLP_requirements_Assign3.txt (line 2)) (2.21)\n",
            "Installing collected packages: wasabi, tensorboard-plugin-wit, pure-eval, nbimporter, keras, flatbuffers, executing, tensorflow-estimator, tensorboard-data-server, qtpy, pydantic, py, protobuf, numpy, jedi, click, asttokens, typer, stack-data, pytest, pandas, keras-preprocessing, thinc, scikit-learn, ipython, google-auth-oauthlib, tensorboard, spacy, tensorflow, qtconsole, jupyter\n",
            "  Attempting uninstall: wasabi\n",
            "    Found existing installation: wasabi 1.1.2\n",
            "    Uninstalling wasabi-1.1.2:\n",
            "      Successfully uninstalled wasabi-1.1.2\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.13.1\n",
            "    Uninstalling keras-2.13.1:\n",
            "      Successfully uninstalled keras-2.13.1\n",
            "  Attempting uninstall: flatbuffers\n",
            "    Found existing installation: flatbuffers 23.5.26\n",
            "    Uninstalling flatbuffers-23.5.26:\n",
            "      Successfully uninstalled flatbuffers-23.5.26\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.13.0\n",
            "    Uninstalling tensorflow-estimator-2.13.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.13.0\n",
            "  Attempting uninstall: tensorboard-data-server\n",
            "    Found existing installation: tensorboard-data-server 0.7.1\n",
            "    Uninstalling tensorboard-data-server-0.7.1:\n",
            "      Successfully uninstalled tensorboard-data-server-0.7.1\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 1.10.12\n",
            "    Uninstalling pydantic-1.10.12:\n",
            "      Successfully uninstalled pydantic-1.10.12\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.20.3\n",
            "    Uninstalling protobuf-3.20.3:\n",
            "      Successfully uninstalled protobuf-3.20.3\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.23.5\n",
            "    Uninstalling numpy-1.23.5:\n",
            "      Successfully uninstalled numpy-1.23.5\n",
            "  Attempting uninstall: click\n",
            "    Found existing installation: click 8.1.7\n",
            "    Uninstalling click-8.1.7:\n",
            "      Successfully uninstalled click-8.1.7\n",
            "  Attempting uninstall: typer\n",
            "    Found existing installation: typer 0.9.0\n",
            "    Uninstalling typer-0.9.0:\n",
            "      Successfully uninstalled typer-0.9.0\n",
            "  Attempting uninstall: pytest\n",
            "    Found existing installation: pytest 7.4.1\n",
            "    Uninstalling pytest-7.4.1:\n",
            "      Successfully uninstalled pytest-7.4.1\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 1.5.3\n",
            "    Uninstalling pandas-1.5.3:\n",
            "      Successfully uninstalled pandas-1.5.3\n",
            "  Attempting uninstall: thinc\n",
            "    Found existing installation: thinc 8.1.12\n",
            "    Uninstalling thinc-8.1.12:\n",
            "      Successfully uninstalled thinc-8.1.12\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.2.2\n",
            "    Uninstalling scikit-learn-1.2.2:\n",
            "      Successfully uninstalled scikit-learn-1.2.2\n",
            "  Attempting uninstall: ipython\n",
            "    Found existing installation: ipython 7.34.0\n",
            "    Uninstalling ipython-7.34.0:\n",
            "      Successfully uninstalled ipython-7.34.0\n",
            "  Attempting uninstall: google-auth-oauthlib\n",
            "    Found existing installation: google-auth-oauthlib 1.0.0\n",
            "    Uninstalling google-auth-oauthlib-1.0.0:\n",
            "      Successfully uninstalled google-auth-oauthlib-1.0.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.13.0\n",
            "    Uninstalling tensorboard-2.13.0:\n",
            "      Successfully uninstalled tensorboard-2.13.0\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 3.6.1\n",
            "    Uninstalling spacy-3.6.1:\n",
            "      Successfully uninstalled spacy-3.6.1\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.13.0\n",
            "    Uninstalling tensorflow-2.13.0:\n",
            "      Successfully uninstalled tensorflow-2.13.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "en-core-web-sm 3.6.0 requires spacy<3.7.0,>=3.6.0, but you have spacy 3.2.4 which is incompatible.\n",
            "google-colab 1.0.0 requires ipython==7.34.0, but you have ipython 8.5.0 which is incompatible.\n",
            "google-colab 1.0.0 requires pandas==1.5.3, but you have pandas 1.3.5 which is incompatible.\n",
            "inflect 7.0.0 requires pydantic>=1.9.1, but you have pydantic 1.8.2 which is incompatible.\n",
            "numba 0.56.4 requires numpy<1.24,>=1.18, but you have numpy 1.24.2 which is incompatible.\n",
            "plotnine 0.12.3 requires pandas>=1.5.0, but you have pandas 1.3.5 which is incompatible.\n",
            "tensorflow-datasets 4.9.2 requires protobuf>=3.20, but you have protobuf 3.19.6 which is incompatible.\n",
            "tensorflow-metadata 1.14.0 requires protobuf<4.21,>=3.20.3, but you have protobuf 3.19.6 which is incompatible.\n",
            "xarray 2023.7.0 requires pandas>=1.4, but you have pandas 1.3.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed asttokens-2.4.0 click-8.0.4 executing-1.2.0 flatbuffers-1.12 google-auth-oauthlib-0.4.6 ipython-8.5.0 jedi-0.19.0 jupyter-1.0.0 keras-2.9.0 keras-preprocessing-1.1.2 nbimporter-0.3.4 numpy-1.24.2 pandas-1.3.5 protobuf-3.19.6 pure-eval-0.2.2 py-1.11.0 pydantic-1.8.2 pytest-7.1.3 qtconsole-5.4.4 qtpy-2.4.0 scikit-learn-1.2.0 spacy-3.2.4 stack-data-0.6.2 tensorboard-2.9.1 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.9.2 tensorflow-estimator-2.9.0 thinc-8.0.17 typer-0.4.2 wasabi-0.10.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "IPython",
                  "google",
                  "numpy"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "\n",
        "def get_package_version(command):\n",
        "    try:\n",
        "        version = subprocess.getoutput(command)\n",
        "        return version\n",
        "    except Exception as e:\n",
        "        return str(e)\n",
        "\n",
        "commands = {\n",
        "    'IPython': 'ipython --version',\n",
        "    'Jupyter': 'pip show jupyter | grep Version',\n",
        "    'Nbimporter': 'pip show nbimporter | grep Version',\n",
        "    'Pytest': 'pip show pytest | grep Version',\n",
        "    'Pandas': 'pip show pandas | grep Version',\n",
        "    'Spacy': 'pip show spacy | grep Version',\n",
        "    'TensorFlow': 'pip show tensorflow | grep Version',\n",
        "    'Numpy': 'pip show numpy | grep Version',\n",
        "    'scikit-learn': 'pip show scikit-learn | grep Version',\n",
        "\n",
        "}\n",
        "\n",
        "for package, command in commands.items():\n",
        "    version = get_package_version(command)\n",
        "    print(f'{package}: {version}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cRQ6odl1qllo",
        "outputId": "bb1893ec-d9c9-4df7-aebe-62b04e5c9217"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "IPython: 8.5.0\n",
            "Jupyter: Version: 1.0.0\n",
            "Nbimporter: Version: 0.3.4\n",
            "Pytest: Version: 7.1.3\n",
            "Pandas: Version: 1.3.5\n",
            "Spacy: Version: 3.2.4\n",
            "TensorFlow: Version: 2.9.2\n",
            "Numpy: Version: 1.24.2\n",
            "scikit-learn: Version: 1.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove.6B.zip -d /content/drive/MyDrive/AIML/data_Assign3/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vUfoFf2CtPQ-",
        "outputId": "fa9be0a0-a475-40c3-828f-150079be93cc"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-09-27 13:50:58--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2023-09-27 13:50:58--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2023-09-27 13:50:58--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  5.00MB/s    in 2m 39s  \n",
            "\n",
            "2023-09-27 13:53:37 (5.17 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n",
            "Archive:  glove.6B.zip\n",
            "replace /content/drive/MyDrive/AIML/data_Assign3/glove.6B.50d.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n",
            "  inflating: /content/drive/MyDrive/AIML/data_Assign3/glove.6B.50d.txt  \n",
            "  inflating: /content/drive/MyDrive/AIML/data_Assign3/glove.6B.100d.txt  \n",
            "  inflating: /content/drive/MyDrive/AIML/data_Assign3/glove.6B.200d.txt  \n",
            "  inflating: /content/drive/MyDrive/AIML/data_Assign3/glove.6B.300d.txt  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "deletable": false,
        "editable": false,
        "tags": [
          "imports"
        ],
        "id": "cTgxw_Hv3kvU"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tensorflow.keras.utils import set_random_seed\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional, TimeDistributed\n",
        "from tensorflow.keras.initializers import Constant\n",
        "from sklearn.metrics import classification_report"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "Fev5y1hq3kvX"
      },
      "source": [
        "When working with Neural Networks, there are a large number of random operations such as initializing the weights of the network, shuffling the data for training, or choosing samples. This causes that different training runs of the same model can lead to different results. To ensure reproducibility, i.e. obtaining the same results in the different runs, the random number generator must be initialized with a fixed value known as seed. In `Keras`, this can be done as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "uFi4i-vW3kvX"
      },
      "outputs": [],
      "source": [
        "seed = 42\n",
        "set_random_seed(seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "gqE8uXrH3kvY"
      },
      "source": [
        "When developing a model, if the results you get are not as expected, try re-initializing the seed by running the cell above before compiling and training the model.\n",
        "\n",
        "> **Note!** With models as complex as Neural Networks, reproducibility is susceptible to factors such as software versions or the hardware on which the models are run. Even with seed initialization, there may be slight differences in the results.\n",
        "\n",
        "Working with Neural Networks also involves defining a number of hyperparameters that set the configuration of the model. Finding the appropriate hyperparameter values requires training the model with different combinations and testing them on the development set. This hyperparameter tuning is a costly process that needs multiple rounds of experimentation. However, for this assignments, you will use the following values:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "RY_0DO7u3kvZ"
      },
      "outputs": [],
      "source": [
        "maxlen = 130  # Maximum length of the input sequence accepted by the model\n",
        "epochs = 6  # Number of epochs to train the model\n",
        "batch_size = 64  # Number of examples used per gradient update\n",
        "embedding_dim = 300  # Dimension of the embeddings\n",
        "rnn_units = 256  # Number of units per RNN layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "10wQp_6O3kva"
      },
      "source": [
        "Training a Deep Learning model with a large train set can be a time-consuming process, as the model needs to iterate over the entire set multiple times, often requiring significant computational resources. During the implementation of the model, it is often a good practice to use only a subset of the training data. This allows a faster debugging of the code. Set the `shrink_dataset` variable as `True` when a faster training is required and set it as `False` to train the model on the whole train set:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "JmLsM2qY3kva"
      },
      "outputs": [],
      "source": [
        "shrink_dataset = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "0Y3EOqrw3kvb"
      },
      "source": [
        "Although the value of this variable does not affect the tests that will evaluate your code, the output examples distributed throughout this notebook are based on a `shrink_dataset` variable set as `False`.\n",
        "\n",
        "The train set for the assignment consists of 248 articles with 1366 sentences in total. The test set contains 136 articles with 848 sentences. A development set with 65 documents and 459 sentences is also provided. The dataset is annotated at the token level following a BIO schema with 3 labels: *B-Quantity*, *I-Quantity* and *O*.  The dataset can be loaded into three `DataFrames` as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "yKAuq9L53kvc"
      },
      "outputs": [],
      "source": [
        "def load_data(data_path, shrink_dataset, seed):\n",
        "    data = pd.read_csv(data_path, sep=\"\\t\", encoding=\"utf8\").dropna()\n",
        "    if shrink_dataset:\n",
        "        sample = data[[\"docId\",  \"sentId\"]].drop_duplicates().sample(frac=0.2, random_state=seed)\n",
        "        data = pd.merge(data, sample, on=[\"docId\", \"sentId\"])\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "US-mGMnF3kvd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 582
        },
        "outputId": "d94dfb19-4f6d-42ab-aaea-2a9e5a04eebe"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                        docId  sentId         word       lemma       label\n",
              "22276  S0378383912000130-3601       3          The         the           O\n",
              "22277  S0378383912000130-3601       3  experiments  experiment           O\n",
              "22278  S0378383912000130-3601       3     involved     involve           O\n",
              "22279  S0378383912000130-3601       3          two         two  B-Quantity\n",
              "22280  S0378383912000130-3601       3        beach       beach  I-Quantity\n",
              "22281  S0378383912000130-3601       3    materials    material  I-Quantity\n",
              "22282  S0378383912000130-3601       3         with        with           O\n",
              "22283  S0378383912000130-3601       3      nominal     nominal           O\n",
              "22284  S0378383912000130-3601       3     sediment    sediment           O\n",
              "22285  S0378383912000130-3601       3    diameters    diameter           O\n",
              "22286  S0378383912000130-3601       3           of          of           O\n",
              "22287  S0378383912000130-3601       3          1.5         1.5  B-Quantity\n",
              "22288  S0378383912000130-3601       3           mm          mm  I-Quantity\n",
              "22289  S0378383912000130-3601       3          and         and  I-Quantity\n",
              "22290  S0378383912000130-3601       3          8.5         8.5  I-Quantity\n",
              "22291  S0378383912000130-3601       3           mm          mm  I-Quantity\n",
              "22292  S0378383912000130-3601       3            .           .           O"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ba0cf317-6b5e-4287-a08d-fc95abfb76c0\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>docId</th>\n",
              "      <th>sentId</th>\n",
              "      <th>word</th>\n",
              "      <th>lemma</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>22276</th>\n",
              "      <td>S0378383912000130-3601</td>\n",
              "      <td>3</td>\n",
              "      <td>The</td>\n",
              "      <td>the</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22277</th>\n",
              "      <td>S0378383912000130-3601</td>\n",
              "      <td>3</td>\n",
              "      <td>experiments</td>\n",
              "      <td>experiment</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22278</th>\n",
              "      <td>S0378383912000130-3601</td>\n",
              "      <td>3</td>\n",
              "      <td>involved</td>\n",
              "      <td>involve</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22279</th>\n",
              "      <td>S0378383912000130-3601</td>\n",
              "      <td>3</td>\n",
              "      <td>two</td>\n",
              "      <td>two</td>\n",
              "      <td>B-Quantity</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22280</th>\n",
              "      <td>S0378383912000130-3601</td>\n",
              "      <td>3</td>\n",
              "      <td>beach</td>\n",
              "      <td>beach</td>\n",
              "      <td>I-Quantity</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22281</th>\n",
              "      <td>S0378383912000130-3601</td>\n",
              "      <td>3</td>\n",
              "      <td>materials</td>\n",
              "      <td>material</td>\n",
              "      <td>I-Quantity</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22282</th>\n",
              "      <td>S0378383912000130-3601</td>\n",
              "      <td>3</td>\n",
              "      <td>with</td>\n",
              "      <td>with</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22283</th>\n",
              "      <td>S0378383912000130-3601</td>\n",
              "      <td>3</td>\n",
              "      <td>nominal</td>\n",
              "      <td>nominal</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22284</th>\n",
              "      <td>S0378383912000130-3601</td>\n",
              "      <td>3</td>\n",
              "      <td>sediment</td>\n",
              "      <td>sediment</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22285</th>\n",
              "      <td>S0378383912000130-3601</td>\n",
              "      <td>3</td>\n",
              "      <td>diameters</td>\n",
              "      <td>diameter</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22286</th>\n",
              "      <td>S0378383912000130-3601</td>\n",
              "      <td>3</td>\n",
              "      <td>of</td>\n",
              "      <td>of</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22287</th>\n",
              "      <td>S0378383912000130-3601</td>\n",
              "      <td>3</td>\n",
              "      <td>1.5</td>\n",
              "      <td>1.5</td>\n",
              "      <td>B-Quantity</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22288</th>\n",
              "      <td>S0378383912000130-3601</td>\n",
              "      <td>3</td>\n",
              "      <td>mm</td>\n",
              "      <td>mm</td>\n",
              "      <td>I-Quantity</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22289</th>\n",
              "      <td>S0378383912000130-3601</td>\n",
              "      <td>3</td>\n",
              "      <td>and</td>\n",
              "      <td>and</td>\n",
              "      <td>I-Quantity</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22290</th>\n",
              "      <td>S0378383912000130-3601</td>\n",
              "      <td>3</td>\n",
              "      <td>8.5</td>\n",
              "      <td>8.5</td>\n",
              "      <td>I-Quantity</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22291</th>\n",
              "      <td>S0378383912000130-3601</td>\n",
              "      <td>3</td>\n",
              "      <td>mm</td>\n",
              "      <td>mm</td>\n",
              "      <td>I-Quantity</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22292</th>\n",
              "      <td>S0378383912000130-3601</td>\n",
              "      <td>3</td>\n",
              "      <td>.</td>\n",
              "      <td>.</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ba0cf317-6b5e-4287-a08d-fc95abfb76c0')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-ba0cf317-6b5e-4287-a08d-fc95abfb76c0 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-ba0cf317-6b5e-4287-a08d-fc95abfb76c0');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-fb0d617b-a932-4b78-83a9-611d3387894c\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-fb0d617b-a932-4b78-83a9-611d3387894c')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-fb0d617b-a932-4b78-83a9-611d3387894c button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "train_data = load_data(\"/content/drive/MyDrive/AIML/data_Assign3/train.tsv\", shrink_dataset, seed)\n",
        "dev_data = load_data(\"/content/drive/MyDrive/AIML/data_Assign3/trial.tsv\", shrink_dataset, seed)\n",
        "test_data = load_data(\"/content/drive/MyDrive/AIML/data_Assign3/eval.tsv\", shrink_dataset, seed)\n",
        "train_data[(train_data.docId == \"S0378383912000130-3601\") & (train_data.sentId == 3)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "veKfPWzA3kvd"
      },
      "source": [
        "The `DataFrames` created include the lemmatization of words in the `lemma` columns. You will use the lemmas as the input of the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "OM5BHaJz3kvd"
      },
      "source": [
        "## Data Pre-processing\n",
        "\n",
        "In this assignment, you will have to implement some steps to pre-process and obtain a representation of the data. You will implement a model with an `Embedding` lookup table as the input layer, so the tokens of the input sentences should be represented as indexes. The target labels should also be represented in similar way. Besides, as one would expect, the sentences in the **MeasEval** dataset have different lengths. However, the input for a Deep Learning model is a batch of examples (in this case, sentences) in the form of a single tensor which requires that all examples in the batch must have the same length. Therefore, the sentences should be padded or truncated to a specific length.\n",
        "\n",
        "> **Note!** For this particular task, the `maxlen` value provided to you guarantees that padding is sufficient to make all sentences the same length without the need for truncation.\n",
        "\n",
        "This first of these pre-processing steps will be to obtain both a vocabulary and the set of labels from the train set. The vocabulary should be the list of unique lemmas and must include the special tokens `[PAD]`, that will be used for padding the sequences, and `[UNK]`, that will be used to represent out-of-vocabulary words. Along with the vocabulary and the label set, you will also have to build a dictionary mapping each lemma to its position in the vocabulary and a dictionary mapping each label to its position in the label set. These dictionaries will be used later to obtain the representation of the input and output of the model. The text is already tokenized and lemmatized which will help in this task.\n",
        "\n",
        "You must complete the code for the `get_vocabulary` function that takes as input the `DataFrame` containing the train set. The function should create a list with the all the unique lemmas and include the special tokens `[PAD]` and `[UNK]` in the first two positions. Similarly, the function should create a list with the unique labels with the special token `[PAD]` in the first position. The **pandas** library provides some [functions](https://pandas.pydata.org/docs/reference/index.html) that may help you. Along with those lists, `get_vocabulary` should return the dictionaries mapping the lemmas and the labels to their corresponding positions. In total, the vocabulary and the label set should have 5508 and 4 items respectively:\n",
        "\n",
        "> Vocabulary size: 5508  \n",
        "Vocabulary first 5 lemmas: ['[PAD]', '[UNK]', 'datum', 'be', 'draw']  \n",
        "Vocabulary dictionary: {'[PAD]': 0, '[UNK]': 1, 'datum': 2, 'be': 3, 'draw': 4}  \n",
        ">\n",
        ">Labels size: 4  \n",
        "Labels: ['[PAD]', 'O', 'B-Quantity', 'I-Quantity']  \n",
        "Labels dictionary: {'[PAD]': 0, 'O': 1, 'B-Quantity': 2, 'I-Quantity': 3}  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **test_get_vocabulary = 3 Marks**"
      ],
      "metadata": {
        "id": "b9lLR7tGEWSZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "tags": [
          "get_vocabulary"
        ],
        "id": "TiIXRYVD3kvd"
      },
      "outputs": [],
      "source": [
        "def get_vocabulary(train_data):\n",
        "    #\n",
        "    #  REPLACE THE pass STATEMENT WITH YOUR CODE\n",
        "    #\n",
        "    # Initialize lists for vocabulary and labels\n",
        "    vocab = ['[PAD]', '[UNK]']\n",
        "    labels = ['[PAD]']\n",
        "\n",
        "    # Initialize dictionaries to map lemmas and labels to positions\n",
        "    word2idx = {'[PAD]': 0, '[UNK]': 1}\n",
        "    label2idx = {'[PAD]': 0}\n",
        "\n",
        "    # Initialize lists for unique lemmas and labels as lists\n",
        "    unique_lemmas = list(train_data['lemma'].unique())\n",
        "    unique_labels = list(train_data['label'].unique())\n",
        "\n",
        "    # Add unique lemmas to the vocabulary\n",
        "    vocab.extend(unique_lemmas)\n",
        "\n",
        "    # Add unique labels to the labels list\n",
        "    labels.extend(unique_labels)\n",
        "\n",
        "    # Create dictionaries to map lemmas and labels to positions\n",
        "    word2idx.update({lemma: idx for idx, lemma in enumerate(vocab)})\n",
        "    label2idx.update({label: idx for idx, label in enumerate(labels)})\n",
        "\n",
        "    return vocab, word2idx, labels, label2idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "StSo9npQ3kvf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf46f661-2dec-4a50-d40b-8aac33ee9e96"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 5508\n",
            "Vocabulary first 5 words: ['[PAD]', '[UNK]', 'datum', 'be', 'draw']\n",
            "Vocabulary dictionary: {'[PAD]': 0, '[UNK]': 1, 'datum': 2, 'be': 3, 'draw': 4}\n",
            "\n",
            "Labels size: 4\n",
            "Labels: ['[PAD]', 'O', 'B-Quantity', 'I-Quantity']\n",
            "Labels dictionary: {'[PAD]': 0, 'O': 1, 'B-Quantity': 2, 'I-Quantity': 3}\n"
          ]
        }
      ],
      "source": [
        "vocab, word2idx, labels, label2idx = get_vocabulary(train_data)\n",
        "vocab_size = len(vocab)\n",
        "label_size = len(labels)\n",
        "print(f\"Vocabulary size: {vocab_size}\")\n",
        "print(f\"Vocabulary first 5 words: {vocab[:5]}\")\n",
        "print(f\"Vocabulary dictionary: { {w: word2idx[w] for w in vocab[:5]}}\")\n",
        "print(\"\")\n",
        "print(f\"Labels size: {label_size}\")\n",
        "print(f\"Labels: {labels}\")\n",
        "print(f\"Labels dictionary: {label2idx}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "KcOJxtef3kvg"
      },
      "source": [
        "Since the *Quantity* recognition task is a Sequence Labeling problem, the input for the model must be the sequence of lemmas in the sentence and the output the sequence of labels. Therefore, the train, development and test `DataFrames` must be reformated by aggregating the data corresponding to each sentence. The `integrate_sentences` will do this for you. The output of `integrate_sentences` is a `DataFrame` with a row for each sentence and the columns `lemmas` and `labels` that contain the list of lemmas and the list of labels of the sentences respectively.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "deletable": false,
        "editable": false,
        "tags": [
          "sentence_integrate"
        ],
        "id": "g_io0LoL3kvg"
      },
      "outputs": [],
      "source": [
        "def integrate_sentences(data):\n",
        "    agg_func = lambda s: [s['lemma'].values.tolist(), s['label'].values.tolist()]\n",
        "    data = data.groupby([\"docId\", \"sentId\"], sort=False).apply(agg_func).reset_index().rename(columns={0: 'lemmas_labels'})\n",
        "    data['lemmas'] = data.apply(lambda x: x['lemmas_labels'][0], axis=1)\n",
        "    data['labels'] = data.apply(lambda x: x['lemmas_labels'][1], axis=1)\n",
        "    data = data.drop(columns=\"lemmas_labels\")\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "LqWFA7Ln3kvg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 98
        },
        "outputId": "96294c20-c514-4089-e9cf-8801eba45ab9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                      docId  sentId  \\\n",
              "767  S0378383912000130-3601       3   \n",
              "\n",
              "                                                                                                                lemmas  \\\n",
              "767  [the, experiment, involve, two, beach, material, with, nominal, sediment, diameter, of, 1.5, mm, and, 8.5, mm, .]   \n",
              "\n",
              "                                                                                                                          labels  \n",
              "767  [O, O, O, B-Quantity, I-Quantity, I-Quantity, O, O, O, O, O, B-Quantity, I-Quantity, I-Quantity, I-Quantity, I-Quantity, O]  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-27d9bdb3-aaae-46c1-8f3c-a300cd50e9ba\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>docId</th>\n",
              "      <th>sentId</th>\n",
              "      <th>lemmas</th>\n",
              "      <th>labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>767</th>\n",
              "      <td>S0378383912000130-3601</td>\n",
              "      <td>3</td>\n",
              "      <td>[the, experiment, involve, two, beach, material, with, nominal, sediment, diameter, of, 1.5, mm, and, 8.5, mm, .]</td>\n",
              "      <td>[O, O, O, B-Quantity, I-Quantity, I-Quantity, O, O, O, O, O, B-Quantity, I-Quantity, I-Quantity, I-Quantity, I-Quantity, O]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-27d9bdb3-aaae-46c1-8f3c-a300cd50e9ba')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-27d9bdb3-aaae-46c1-8f3c-a300cd50e9ba button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-27d9bdb3-aaae-46c1-8f3c-a300cd50e9ba');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "train_examples = integrate_sentences(train_data)\n",
        "dev_examples = integrate_sentences(dev_data)\n",
        "test_examples = integrate_sentences(test_data)\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "train_examples[(train_examples.docId == \"S0378383912000130-3601\") & (train_examples.sentId == 3)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EslW3eIM3kvh"
      },
      "source": [
        "The dataset is now ready for you to get the numerical representation of both input and output. You must perform two steps to process the sequence of lemmas and the sequence of labels:\n",
        "1. For each sentence, translate each lemma or label to its corresponding index using the `word2idx` and `label2idx` dictionaries. In case the lemma is not found in `word2idx`, use the index of the `[UNK]` token instead.\n",
        "2. Pad both the sequences of lemmas and the sequences of labels to the same length as defined by the `maxlen` variable. For this, you should use the [pad_sequences](https://www.tensorflow.org/api_docs/python/tf/keras/utils/pad_sequences) function with its default padding strategy. This function uses `0` as the default padding value which corresponds to the index of the `[PAD]` token in the vocabulary.   \n",
        "\n",
        "You must complete the code for the `format_examples` function. This function takes as input a `DataFrame` in the format returned by `integrate_sentences`, the `word2idx` and `label2idx` dictionaries, and the `maxlen` variable. The function must run the steps described above and return a **numpy** array with the processed lemma sequences and a **numpy** array with the processed label sequences that will be used as input and output of the model respectively. Applying `format_examples` to the train, development and test sets should result on 6 arrays with the following shapes:\n",
        "\n",
        ">Shape of train input data :  (1366, 130)  \n",
        "Shape of train output data :  (1366, 130)  \n",
        "Shape of development input data :  (459, 130)  \n",
        "Shape of development output data :  (459, 130)  \n",
        "Shape of test input data :  (848, 130)  \n",
        "Shape of test output data :  (848, 130)  "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **test_format_examples = 3 Marks**"
      ],
      "metadata": {
        "id": "ifVT0qJlM1_P"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "tags": [
          "format_examples"
        ],
        "id": "-NOYMzSN3kvh"
      },
      "outputs": [],
      "source": [
        "def format_examples(data, word2idx, label2idx, maxlen):\n",
        "    #\n",
        "    #  REPLACE THE pass STATEMENT WITH YOUR CODE\n",
        "    #\n",
        "    # Initialize empty lists to store processed lemma and label sequences\n",
        "    processed_lemmas = []\n",
        "    processed_labels = []\n",
        "\n",
        "    # Iterate through each row in the DataFrame\n",
        "    for index, row in data.iterrows():\n",
        "        # Translate lemmas to their corresponding indices, use [UNK] index if not found\n",
        "        lemma_indices = [word2idx.get(lemma, word2idx['[UNK]']) for lemma in row['lemmas']]\n",
        "\n",
        "        # Translate labels to their corresponding indices\n",
        "        label_indices = [label2idx[label] for label in row['labels']]\n",
        "\n",
        "        # Append the processed sequences to the lists\n",
        "        processed_lemmas.append(lemma_indices)\n",
        "        processed_labels.append(label_indices)\n",
        "\n",
        "    # Pad the lemma and label sequences to the specified maxlen\n",
        "    x_data = pad_sequences(processed_lemmas, maxlen=maxlen, padding='post', value=0)  # 0 corresponds to [PAD]\n",
        "    y_data = pad_sequences(processed_labels, maxlen=maxlen, padding='post', value=0)  # 0 corresponds to [PAD]\n",
        "\n",
        "    return x_data, y_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "SXhHrMi-3kvi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12541c39-cf41-41e1-a9e5-5e3d56fbc332"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of train input data:  (1366, 130)\n",
            "Shape of train output data:  (1366, 130)\n",
            "Shape of development input data:  (459, 130)\n",
            "Shape of development output data:  (459, 130)\n",
            "Shape of test input data:  (848, 130)\n",
            "Shape of test output data:  (848, 130)\n"
          ]
        }
      ],
      "source": [
        "x_train, y_train = format_examples(train_examples, word2idx, label2idx, maxlen)\n",
        "x_dev, y_dev = format_examples(dev_examples, word2idx, label2idx, maxlen)\n",
        "x_test, y_test = format_examples(test_examples, word2idx, label2idx, maxlen)\n",
        "print(\"Shape of train input data: \", x_train.shape)\n",
        "print(\"Shape of train output data: \", y_train.shape)\n",
        "print(\"Shape of development input data: \", x_dev.shape)\n",
        "print(\"Shape of development output data: \", y_dev.shape)\n",
        "print(\"Shape of test input data: \", x_test.shape)\n",
        "print(\"Shape of test output data: \", y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "_yPowN8i3kvi"
      },
      "source": [
        "## Recurrent Neural Network\n",
        "\n",
        "There are three ways to create a neural network with **Kerars**: using the [Functional API](https://keras.io/guides/functional_api/), by [Model subclassing](https://keras.io/guides/making_new_layers_and_models_via_subclassing/) or creating a [Sequential model](https://keras.io/guides/sequential_model/). In this assignment, you will use the latter option. A `Sequential` model is a straightforward approach to build simple neural networks by stacking the layers. You will construct a RNN with the following 3 layers:\n",
        "1. An [Embedding](https://keras.io/api/layers/core_layers/embedding/) layer with an input dimension equal to the vocabulary size, an embedding dimension defined by `embedding_dim` and where the length of the input sequences is equal to `maxlen`. The layer must also mask out the padding values so that they are not considered when computing the loss.\n",
        "2. A Bidirectional [LSTM](https://keras.io/api/layers/recurrent_layers/lstm/) layer with a number of units determined by `rnn_units`. Since you are working on Sequence Labeling, the `LSTM` must return outputs for the full sequence. To make it Bidirectional, the `LSTM` must be wrapped by a [Bidirectional](https://keras.io/api/layers/recurrent_layers/bidirectional/) layer.\n",
        "3. A [Dense](https://keras.io/api/layers/core_layers/dense/) layer with a number of units equal to the number of labels and a `softmax` activation function.  Since you are working on Sequence Labeling, the `Dense` layer must be wrapped by a [TimeDistributed](https://keras.io/api/layers/recurrent_layers/time_distributed/) layer.\n",
        "\n",
        "You must complete the code for the `create_model` function. This function takes as input the size of the vocabulary, the number of labels and the `maxlen`, `embedding_dim` and `rnn_units` hyperparameters. The function must create a RNN according to the configuration described above. Read carefully all the linked documentation to learn how to create such a model.  Any option not mentioned in the description should be kept with its default value. The summary of the resulting model should look like:\n",
        "\n",
        "\n",
        "> <pre>\n",
        "> Model: \"sequential_1\"\n",
        "> __________________________________________________________________________________________\n",
        "> Layer (type)                           Output Shape                        Param #       \n",
        "> ==========================================================================================\n",
        "> embedding_1 (Embedding)               (None, 130, 300)                    1652400       \n",
        ">                                                                                          \n",
        "> bidirectional_1 (Bidirectional)       (None, 130, 512)                    1140736       \n",
        ">                                                                                          \n",
        "> time_distributed_1 (TimeDistributed)  (None, 130, 4)                      2052          \n",
        ">                                                                                          \n",
        "> ==========================================================================================\n",
        "> Total params: 2,795,188\n",
        "> Trainable params: 2,795,188\n",
        "> Non-trainable params: 0\n",
        "> __________________________________________________________________________________________\n",
        "> </pre>\n",
        "\n",
        "Before returning the model, the `create_model` function should [compile](https://keras.io/api/models/model_training_apis/#compile-method) it using `'sparse_categorical_crossentropy'` as the loss function, `'adam'` as the optimizer and `'sparse_categorical_accuracy'` as a metric to evaluate the model during training."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **test_create_model = 3 Marks**"
      ],
      "metadata": {
        "id": "ArFxR31qNjmA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "tags": [
          "create_model"
        ],
        "id": "zjoEWfqc3kvj"
      },
      "outputs": [],
      "source": [
        "def create_model(vocab_size, label_size, maxlen, embedding_dim, rnn_units):\n",
        "    #\n",
        "    #  REPLACE THE pass STATEMENT WITH YOUR CODE\n",
        "    #\n",
        "    # Create a Sequential model\n",
        "    model = Sequential()\n",
        "\n",
        "    # Add an Embedding layer with mask_zero=True to handle padding values\n",
        "    model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=maxlen, mask_zero=True))\n",
        "\n",
        "    # Add a Bidirectional LSTM layer\n",
        "    model.add(Bidirectional(LSTM(units=rnn_units, return_sequences=True)))\n",
        "\n",
        "    # Add a TimeDistributed Dense layer for sequence labeling\n",
        "    model.add(TimeDistributed(Dense(units=label_size, activation='softmax')))\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['sparse_categorical_accuracy'])\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "YCfaRURB3kvk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "563c81d3-ad5d-4c86-a3aa-c1994f8af7da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "__________________________________________________________________________________________\n",
            " Layer (type)                           Output Shape                        Param #       \n",
            "==========================================================================================\n",
            " embedding (Embedding)                  (None, 130, 300)                    1652400       \n",
            "                                                                                          \n",
            " bidirectional (Bidirectional)          (None, 130, 512)                    1140736       \n",
            "                                                                                          \n",
            " time_distributed (TimeDistributed)     (None, 130, 4)                      2052          \n",
            "                                                                                          \n",
            "==========================================================================================\n",
            "Total params: 2,795,188\n",
            "Trainable params: 2,795,188\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = create_model(vocab_size, label_size, maxlen, embedding_dim, rnn_units)\n",
        "model.summary(line_length=90)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "pumghIqn3kvl"
      },
      "source": [
        "Once the data has been processed and the model has been compiled, you can proceed to train it.\n",
        "\n",
        "You must complete the `train_model` function. The function takes as input the model created by `create_model`, the train input and output obtained by `format_examples` as well as the development input and output produced by the same function. The function also takes the `batch_size` and `epochs` hyperparameters. The function should train the model on the training data using those hyperparameters. During the training, `train_model` should evaluate the loss and any model metrics on the development data. With `shrink_dataset = False`, the training will take several minutes."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **test_train_model= 4 Marks**"
      ],
      "metadata": {
        "id": "kq3IeI6hNyK2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "tags": [
          "train_model"
        ],
        "id": "i5vqrOZG3kvl"
      },
      "outputs": [],
      "source": [
        "def train_model(model, x_train, y_train, x_dev, y_dev, batch_size, epochs):\n",
        "    #\n",
        "    #  REPLACE THE pass STATEMENT WITH YOUR CODE\n",
        "    #\n",
        "    # Train the model\n",
        "    history = model.fit(\n",
        "        x_train,  # Training input data\n",
        "        y_train,  # Training output data\n",
        "        validation_data=(x_dev, y_dev),  # Development data for validation\n",
        "        batch_size=batch_size,\n",
        "        epochs=epochs,\n",
        "        verbose=1  # Show training progress\n",
        "    )\n",
        "\n",
        "    return history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "dwtu39OG3kvl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81f51201-2392-47c2-b164-98c2a2fc30ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/6\n",
            "22/22 [==============================] - 106s 4s/step - loss: 0.1310 - sparse_categorical_accuracy: 0.9045 - val_loss: 0.0648 - val_sparse_categorical_accuracy: 0.9454\n",
            "Epoch 2/6\n",
            "22/22 [==============================] - 66s 3s/step - loss: 0.0557 - sparse_categorical_accuracy: 0.9418 - val_loss: 0.0516 - val_sparse_categorical_accuracy: 0.9448\n",
            "Epoch 3/6\n",
            "22/22 [==============================] - 65s 3s/step - loss: 0.0407 - sparse_categorical_accuracy: 0.9442 - val_loss: 0.0357 - val_sparse_categorical_accuracy: 0.9510\n",
            "Epoch 4/6\n",
            "22/22 [==============================] - 64s 3s/step - loss: 0.0255 - sparse_categorical_accuracy: 0.9601 - val_loss: 0.0267 - val_sparse_categorical_accuracy: 0.9639\n",
            "Epoch 5/6\n",
            "22/22 [==============================] - 63s 3s/step - loss: 0.0168 - sparse_categorical_accuracy: 0.9743 - val_loss: 0.0246 - val_sparse_categorical_accuracy: 0.9680\n",
            "Epoch 6/6\n",
            "22/22 [==============================] - 62s 3s/step - loss: 0.0125 - sparse_categorical_accuracy: 0.9809 - val_loss: 0.0227 - val_sparse_categorical_accuracy: 0.9694\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7882694420e0>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "train_model(model, x_train, y_train, x_dev, y_dev, batch_size, epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "Ye99Vj8z3kvl"
      },
      "source": [
        "After training, the model can be used to make predictions on unlabeled data using the [predict](https://keras.io/api/models/model_training_apis/#predict-method) method.\n",
        "\n",
        "You must complete the code for the `make_predictions` function. The functions takes as input the model already trained, the test input data produced by `format_examples` and the `batch_size` hyperparameter. The function must run the `predict` method on the input data using batches of size equal to `batch_size`. The `predict` method will return a **numpy** array with 3 axes: `(number of sentences, maxlen, label_size)`. For each token in each sentence, `predict` returns a vector with the probabilities predicted for every label. The output of `make_predictions` must include only the index of the label with the highest probability for each token. For example, if the prediction for one token is the vector `[0.04974193, 0.1511916, 0.65180656, 0.14725993]`, the output for that token should be `2`. For this, you can apply the [argmax](https://numpy.org/doc/stable/reference/generated/numpy.argmax.html) method along the last axis of the **numpy** array."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **test_make_predictions= 3 Marks**"
      ],
      "metadata": {
        "id": "-hGPoXqrOA0f"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "tags": [
          "make_predictions"
        ],
        "id": "kiOJBgvV3kvl"
      },
      "outputs": [],
      "source": [
        "def make_predictions(model, x_test, batch_size):\n",
        "    #\n",
        "    #  REPLACE THE pass STATEMENT WITH YOUR CODE\n",
        "    #\n",
        "    # Make predictions on the test data in batches\n",
        "    predictions = model.predict(x_test, batch_size=batch_size, verbose=1)\n",
        "\n",
        "    # Get the index of the label with the highest probability for each token\n",
        "    predicted_labels = predictions.argmax(axis=-1)\n",
        "\n",
        "    return predicted_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "QU9FqsZk3kvm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09e27c82-9b67-4258-eb2f-4a73bf044349"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "14/14 [==============================] - 13s 516ms/step\n"
          ]
        }
      ],
      "source": [
        "predictions = make_predictions(model, x_test, batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "G_UrXssR3kvm"
      },
      "source": [
        "Since the predictions are now label indexes, they can be translated to the corresponding label by accessing the `labels` list. The `predictions_to_labels` functions iterates over all the sequences in the test set and translates the prediction of each token to the corresponding label. The function skips the padding tokens. The new format of the predictions can be stored in the `prediction` column of the test `DataFrame`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "EadjzW043kvm"
      },
      "outputs": [],
      "source": [
        "def predictions_to_labels(predictions, x_test, labels):\n",
        "    pred_labels = []\n",
        "    for pred_seq, x_seq in zip(predictions, x_test):\n",
        "        pred_seq_labels = [labels[p] for p, x in zip(pred_seq, x_seq) if x!=0]\n",
        "        pred_labels.extend(pred_seq_labels)\n",
        "    return pred_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "Ktc0FHDb3kvm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3ae16b8d-1e63-4733-b415-5a8aaf5ca7bc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                       docId  sentId           word          lemma  \\\n",
              "7328  S0038071711004354-1624       2  Approximately  approximately   \n",
              "7329  S0038071711004354-1624       2             15             15   \n",
              "7330  S0038071711004354-1624       2            min            min   \n",
              "7331  S0038071711004354-1624       2         before         before   \n",
              "7332  S0038071711004354-1624       2            the            the   \n",
              "7333  S0038071711004354-1624       2      beginning      beginning   \n",
              "7334  S0038071711004354-1624       2             of             of   \n",
              "7335  S0038071711004354-1624       2            the            the   \n",
              "7336  S0038071711004354-1624       2     experiment     experiment   \n",
              "7337  S0038071711004354-1624       2              ,              ,   \n",
              "7338  S0038071711004354-1624       2            the            the   \n",
              "7339  S0038071711004354-1624       2    aboveground    aboveground   \n",
              "7340  S0038071711004354-1624       2     vegetation     vegetation   \n",
              "7341  S0038071711004354-1624       2            was             be   \n",
              "7342  S0038071711004354-1624       2        removed         remove   \n",
              "7343  S0038071711004354-1624       2          under          under   \n",
              "7344  S0038071711004354-1624       2            the            the   \n",
              "7345  S0038071711004354-1624       2           dual           dual   \n",
              "7346  S0038071711004354-1624       2              -              -   \n",
              "7347  S0038071711004354-1624       2        chamber        chamber   \n",
              "7348  S0038071711004354-1624       2              ,              ,   \n",
              "7349  S0038071711004354-1624       2             as             as   \n",
              "7350  S0038071711004354-1624       2           well           well   \n",
              "7351  S0038071711004354-1624       2             as             as   \n",
              "7352  S0038071711004354-1624       2            the            the   \n",
              "7353  S0038071711004354-1624       2      reference      reference   \n",
              "7354  S0038071711004354-1624       2        chamber        chamber   \n",
              "7355  S0038071711004354-1624       2          which          which   \n",
              "7356  S0038071711004354-1624       2            was             be   \n",
              "7357  S0038071711004354-1624       2       deployed         deploy   \n",
              "7358  S0038071711004354-1624       2             at             at   \n",
              "7359  S0038071711004354-1624       2             30             30   \n",
              "7360  S0038071711004354-1624       2             cm             cm   \n",
              "7361  S0038071711004354-1624       2       distance       distance   \n",
              "7362  S0038071711004354-1624       2           from           from   \n",
              "7363  S0038071711004354-1624       2            the            the   \n",
              "7364  S0038071711004354-1624       2           dual           dual   \n",
              "7365  S0038071711004354-1624       2              -              -   \n",
              "7366  S0038071711004354-1624       2        chamber        chamber   \n",
              "7367  S0038071711004354-1624       2              .              .   \n",
              "\n",
              "           label  prediction  \n",
              "7328  B-Quantity  B-Quantity  \n",
              "7329  I-Quantity  I-Quantity  \n",
              "7330  I-Quantity  I-Quantity  \n",
              "7331           O           O  \n",
              "7332           O           O  \n",
              "7333           O           O  \n",
              "7334           O           O  \n",
              "7335           O           O  \n",
              "7336           O           O  \n",
              "7337           O           O  \n",
              "7338           O           O  \n",
              "7339           O           O  \n",
              "7340           O           O  \n",
              "7341           O           O  \n",
              "7342           O           O  \n",
              "7343           O           O  \n",
              "7344           O           O  \n",
              "7345           O           O  \n",
              "7346           O           O  \n",
              "7347           O           O  \n",
              "7348           O           O  \n",
              "7349           O           O  \n",
              "7350           O           O  \n",
              "7351           O           O  \n",
              "7352           O           O  \n",
              "7353           O           O  \n",
              "7354           O           O  \n",
              "7355           O           O  \n",
              "7356           O           O  \n",
              "7357           O           O  \n",
              "7358           O           O  \n",
              "7359  B-Quantity  B-Quantity  \n",
              "7360  I-Quantity  I-Quantity  \n",
              "7361           O           O  \n",
              "7362           O           O  \n",
              "7363           O           O  \n",
              "7364           O           O  \n",
              "7365           O           O  \n",
              "7366           O           O  \n",
              "7367           O           O  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-fbc87653-c1d2-4e04-88ed-9f2dda3cead7\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>docId</th>\n",
              "      <th>sentId</th>\n",
              "      <th>word</th>\n",
              "      <th>lemma</th>\n",
              "      <th>label</th>\n",
              "      <th>prediction</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>7328</th>\n",
              "      <td>S0038071711004354-1624</td>\n",
              "      <td>2</td>\n",
              "      <td>Approximately</td>\n",
              "      <td>approximately</td>\n",
              "      <td>B-Quantity</td>\n",
              "      <td>B-Quantity</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7329</th>\n",
              "      <td>S0038071711004354-1624</td>\n",
              "      <td>2</td>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "      <td>I-Quantity</td>\n",
              "      <td>I-Quantity</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7330</th>\n",
              "      <td>S0038071711004354-1624</td>\n",
              "      <td>2</td>\n",
              "      <td>min</td>\n",
              "      <td>min</td>\n",
              "      <td>I-Quantity</td>\n",
              "      <td>I-Quantity</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7331</th>\n",
              "      <td>S0038071711004354-1624</td>\n",
              "      <td>2</td>\n",
              "      <td>before</td>\n",
              "      <td>before</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7332</th>\n",
              "      <td>S0038071711004354-1624</td>\n",
              "      <td>2</td>\n",
              "      <td>the</td>\n",
              "      <td>the</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7333</th>\n",
              "      <td>S0038071711004354-1624</td>\n",
              "      <td>2</td>\n",
              "      <td>beginning</td>\n",
              "      <td>beginning</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7334</th>\n",
              "      <td>S0038071711004354-1624</td>\n",
              "      <td>2</td>\n",
              "      <td>of</td>\n",
              "      <td>of</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7335</th>\n",
              "      <td>S0038071711004354-1624</td>\n",
              "      <td>2</td>\n",
              "      <td>the</td>\n",
              "      <td>the</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7336</th>\n",
              "      <td>S0038071711004354-1624</td>\n",
              "      <td>2</td>\n",
              "      <td>experiment</td>\n",
              "      <td>experiment</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7337</th>\n",
              "      <td>S0038071711004354-1624</td>\n",
              "      <td>2</td>\n",
              "      <td>,</td>\n",
              "      <td>,</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7338</th>\n",
              "      <td>S0038071711004354-1624</td>\n",
              "      <td>2</td>\n",
              "      <td>the</td>\n",
              "      <td>the</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7339</th>\n",
              "      <td>S0038071711004354-1624</td>\n",
              "      <td>2</td>\n",
              "      <td>aboveground</td>\n",
              "      <td>aboveground</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7340</th>\n",
              "      <td>S0038071711004354-1624</td>\n",
              "      <td>2</td>\n",
              "      <td>vegetation</td>\n",
              "      <td>vegetation</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7341</th>\n",
              "      <td>S0038071711004354-1624</td>\n",
              "      <td>2</td>\n",
              "      <td>was</td>\n",
              "      <td>be</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7342</th>\n",
              "      <td>S0038071711004354-1624</td>\n",
              "      <td>2</td>\n",
              "      <td>removed</td>\n",
              "      <td>remove</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7343</th>\n",
              "      <td>S0038071711004354-1624</td>\n",
              "      <td>2</td>\n",
              "      <td>under</td>\n",
              "      <td>under</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7344</th>\n",
              "      <td>S0038071711004354-1624</td>\n",
              "      <td>2</td>\n",
              "      <td>the</td>\n",
              "      <td>the</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7345</th>\n",
              "      <td>S0038071711004354-1624</td>\n",
              "      <td>2</td>\n",
              "      <td>dual</td>\n",
              "      <td>dual</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7346</th>\n",
              "      <td>S0038071711004354-1624</td>\n",
              "      <td>2</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7347</th>\n",
              "      <td>S0038071711004354-1624</td>\n",
              "      <td>2</td>\n",
              "      <td>chamber</td>\n",
              "      <td>chamber</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7348</th>\n",
              "      <td>S0038071711004354-1624</td>\n",
              "      <td>2</td>\n",
              "      <td>,</td>\n",
              "      <td>,</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7349</th>\n",
              "      <td>S0038071711004354-1624</td>\n",
              "      <td>2</td>\n",
              "      <td>as</td>\n",
              "      <td>as</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7350</th>\n",
              "      <td>S0038071711004354-1624</td>\n",
              "      <td>2</td>\n",
              "      <td>well</td>\n",
              "      <td>well</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7351</th>\n",
              "      <td>S0038071711004354-1624</td>\n",
              "      <td>2</td>\n",
              "      <td>as</td>\n",
              "      <td>as</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7352</th>\n",
              "      <td>S0038071711004354-1624</td>\n",
              "      <td>2</td>\n",
              "      <td>the</td>\n",
              "      <td>the</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7353</th>\n",
              "      <td>S0038071711004354-1624</td>\n",
              "      <td>2</td>\n",
              "      <td>reference</td>\n",
              "      <td>reference</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7354</th>\n",
              "      <td>S0038071711004354-1624</td>\n",
              "      <td>2</td>\n",
              "      <td>chamber</td>\n",
              "      <td>chamber</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7355</th>\n",
              "      <td>S0038071711004354-1624</td>\n",
              "      <td>2</td>\n",
              "      <td>which</td>\n",
              "      <td>which</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7356</th>\n",
              "      <td>S0038071711004354-1624</td>\n",
              "      <td>2</td>\n",
              "      <td>was</td>\n",
              "      <td>be</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7357</th>\n",
              "      <td>S0038071711004354-1624</td>\n",
              "      <td>2</td>\n",
              "      <td>deployed</td>\n",
              "      <td>deploy</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7358</th>\n",
              "      <td>S0038071711004354-1624</td>\n",
              "      <td>2</td>\n",
              "      <td>at</td>\n",
              "      <td>at</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7359</th>\n",
              "      <td>S0038071711004354-1624</td>\n",
              "      <td>2</td>\n",
              "      <td>30</td>\n",
              "      <td>30</td>\n",
              "      <td>B-Quantity</td>\n",
              "      <td>B-Quantity</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7360</th>\n",
              "      <td>S0038071711004354-1624</td>\n",
              "      <td>2</td>\n",
              "      <td>cm</td>\n",
              "      <td>cm</td>\n",
              "      <td>I-Quantity</td>\n",
              "      <td>I-Quantity</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7361</th>\n",
              "      <td>S0038071711004354-1624</td>\n",
              "      <td>2</td>\n",
              "      <td>distance</td>\n",
              "      <td>distance</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7362</th>\n",
              "      <td>S0038071711004354-1624</td>\n",
              "      <td>2</td>\n",
              "      <td>from</td>\n",
              "      <td>from</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7363</th>\n",
              "      <td>S0038071711004354-1624</td>\n",
              "      <td>2</td>\n",
              "      <td>the</td>\n",
              "      <td>the</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7364</th>\n",
              "      <td>S0038071711004354-1624</td>\n",
              "      <td>2</td>\n",
              "      <td>dual</td>\n",
              "      <td>dual</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7365</th>\n",
              "      <td>S0038071711004354-1624</td>\n",
              "      <td>2</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7366</th>\n",
              "      <td>S0038071711004354-1624</td>\n",
              "      <td>2</td>\n",
              "      <td>chamber</td>\n",
              "      <td>chamber</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7367</th>\n",
              "      <td>S0038071711004354-1624</td>\n",
              "      <td>2</td>\n",
              "      <td>.</td>\n",
              "      <td>.</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-fbc87653-c1d2-4e04-88ed-9f2dda3cead7')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-fbc87653-c1d2-4e04-88ed-9f2dda3cead7 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-fbc87653-c1d2-4e04-88ed-9f2dda3cead7');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-a2f22e82-fda9-4bf8-b3e1-c493666d3f60\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-a2f22e82-fda9-4bf8-b3e1-c493666d3f60')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-a2f22e82-fda9-4bf8-b3e1-c493666d3f60 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "test_data['prediction'] = predictions_to_labels(predictions, x_test, labels)\n",
        "test_data[(test_data.docId == \"S0038071711004354-1624\") & (test_data.sentId == 2)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "KwAXBX3b3kvn"
      },
      "source": [
        "Although it is not the usual way of evaluating Sequence Labeling tasks, **MeasEval** uses a metric based on the Reading Comprehension *Macro-Averaged F1*. This metric measures the amount of overlapping tokens between the predictions and the true labels. In this assignment, we will approximate this metric by evaluating how many tokens belonging to a *Quantity* are captured by the model. This is done by the `evaluate` function. For the model trained above, the result of this evaluation should look like:\n",
        "\n",
        "> <pre>\n",
        ">               precision    recall  f1-score   support\n",
        ">\n",
        ">     Quantity       0.85      0.69      0.76      1263\n",
        ">\n",
        ">    micro avg       0.85      0.69      0.76      1263\n",
        ">    macro avg       0.85      0.69      0.76      1263\n",
        "> weighted avg       0.85      0.69      0.76      1263\n",
        "> </pre>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "CvJda_FF3kvn"
      },
      "outputs": [],
      "source": [
        "def evaluate(data):\n",
        "    labels = data.apply(lambda x: x['label'].replace(\"B-\", \"\").replace(\"I-\", \"\"), axis=1).values\n",
        "    predictions = data.apply(lambda x: x['prediction'].replace(\"B-\", \"\").replace(\"I-\", \"\"), axis=1).values\n",
        "    print(classification_report(labels, predictions, labels=[\"Quantity\"]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "eEBSX8QO3kvn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4627c5cf-2613-4dc7-a6ac-8b3a48e8294a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Quantity       0.85      0.69      0.76      1263\n",
            "\n",
            "   micro avg       0.85      0.69      0.76      1263\n",
            "   macro avg       0.85      0.69      0.76      1263\n",
            "weighted avg       0.85      0.69      0.76      1263\n",
            "\n"
          ]
        }
      ],
      "source": [
        "evaluate(test_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "Pb9NnwyK3kvn"
      },
      "source": [
        "## Pre-trained Word Embeddings\n",
        "\n",
        "Initializing neural networks with pre-trained word embeddings has a significant impact on many NLP tasks. In the following exercise, you will experiment whether this is also the case for *Quantity* recognition using **GloVe**. You can refer to the following tutorial to learn how to complete this exercise with **keras**: [Using pre-trained word embeddings\n",
        "](https://keras.io/examples/nlp/pretrained_word_embeddings/)\n",
        "\n",
        "\n",
        "First, the `load_embeddings` function will load **GloVe** and return a dictionary mapping words to their embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "8l83XCCp3kvn"
      },
      "outputs": [],
      "source": [
        "def load_embeddings(glove_path):\n",
        "    embedding_index = {}\n",
        "    with open(glove_path, encoding=\"utf8\") as glove_file:\n",
        "        for line in glove_file:\n",
        "            word, coefs = line.split(maxsplit=1)\n",
        "            coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
        "            embedding_index[word] = coefs\n",
        "    return embedding_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "FIOPdTVS3kvo"
      },
      "outputs": [],
      "source": [
        "glove_path = f\"/content/drive/MyDrive/AIML/data_Assign3/glove.6B.{embedding_dim}d.txt\"\n",
        "embedding_index = load_embeddings(glove_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "OBCdc2YK3kvu"
      },
      "source": [
        "To initialize the `Embedding` layer with the **GloVe** embeddings, you have to create a matrix with `(vocab_size, embedding_dim)` dimensions. The *i-th* row in the matrix corresponds to the *i-th* lemma in the vocabulary and contains the **GloVe** embedding for that lemma.\n",
        "\n",
        "You must complete the code for the `create_embedding_matrix` function. The function takes the embedding dictionary created by `load_embeddings`, the vocabulary dictionary, the size of the vocabulary and the `embedding_dim` hyperparameter. The function should initialize a `(vocab_size, embedding_dim)` **numpy** array with zeros and then replace each row with the appropriate **GloVe** embedding if the corresponding lemma exists in the embedding dictionary. For example, the embedding for \"*statistic*\" should exist in the resulting `embedding_matrix`:\n",
        "> <pre>\n",
        "array([ 0.1085    ,  0.82801998,  0.10672   ,  0.0094136 , -0.30441001,\n",
        "        0.75617999, -0.14704999, -0.15469   , -0.97372001, -0.60413003,\n",
        "        0.065233  , -0.055324  , -0.094477  ,  0.23502   ,  0.16466001,\n",
        "        ...\n",
        "</pre>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **test_create_embedding_matrix= 2 Marks**"
      ],
      "metadata": {
        "id": "VyI4zA2aOSam"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "tags": [
          "create_embedding_matrix"
        ],
        "id": "ENtaPzBj3kvu"
      },
      "outputs": [],
      "source": [
        "def create_embedding_matrix(embedding_index, word2idx, vocab_size, embedding_dim):\n",
        "    #\n",
        "    #  REPLACE THE pass STATEMENT WITH YOUR CODE\n",
        "    #\n",
        "    # Initialize the embedding matrix with zeros\n",
        "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "\n",
        "    # Iterate over the vocabulary and set the corresponding GloVe embeddings\n",
        "    for word, idx in word2idx.items():\n",
        "        embedding_vector = embedding_index.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            embedding_matrix[idx] = embedding_vector\n",
        "\n",
        "    return embedding_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "BZTGA6Jh3kvv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "239128e9-d767-45a8-899f-f84b67d14c8e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.1085    ,  0.82801998,  0.10672   ,  0.0094136 , -0.30441001,\n",
              "        0.75617999, -0.14704999, -0.15469   , -0.97372001, -0.60413003,\n",
              "        0.065233  , -0.055324  , -0.094477  ,  0.23502   ,  0.16466001,\n",
              "       -0.043116  ,  0.12965   , -0.48658001,  0.14789   , -0.21789999,\n",
              "        0.53388   ,  0.0022767 , -0.11502   ,  0.50904   ,  0.046882  ,\n",
              "        0.76059002, -0.15012001,  0.070032  ,  0.12826   , -0.26036999,\n",
              "        0.030337  , -0.20457   , -0.73719001, -0.41698   , -0.39225999,\n",
              "        0.10444   , -0.27976999,  0.36368999, -0.53135002, -0.24784   ,\n",
              "        0.72441   ,  0.24203999, -0.089047  , -0.030367  , -0.39888999,\n",
              "        0.019355  , -0.66887999, -0.36210999, -0.41176999,  0.59864998,\n",
              "        0.25565001,  0.38666001, -0.064241  ,  0.22142   , -0.18565001,\n",
              "        0.32170999,  0.20461001, -0.037658  , -0.18867999,  0.06231   ,\n",
              "       -0.33331001, -0.065288  ,  0.36129999,  0.069083  , -0.45622   ,\n",
              "       -0.080719  , -0.19212   ,  0.017178  ,  0.50905001,  0.81054997,\n",
              "       -0.96237999,  0.57374001, -0.68526   ,  0.075411  , -0.53068   ,\n",
              "       -0.047891  ,  0.44292   , -0.42081001, -0.28928   ,  0.16065   ,\n",
              "       -0.30575001,  0.21333   ,  0.62694001, -0.028551  , -0.23056   ,\n",
              "        0.37834001, -0.06101   , -0.10411   ,  0.04182   ,  0.36056   ,\n",
              "       -0.22515   , -0.13383999, -0.21155   , -0.37575001, -0.022031  ,\n",
              "        0.43046999,  0.066295  , -0.03755   ,  0.0179    ,  0.095457  ,\n",
              "        0.32977   ,  0.3082    ,  0.0081692 , -0.16071001,  0.13176   ,\n",
              "       -0.51991999, -0.21731   , -0.47982001, -0.28992999,  0.62965   ,\n",
              "        0.26859   ,  0.093778  , -0.87300003, -0.02335   ,  0.63243002,\n",
              "        0.40823999,  0.45792001,  0.21115001,  0.03629   , -0.046396  ,\n",
              "        0.33298999,  0.45012   ,  0.83485001,  0.70267999, -0.058322  ,\n",
              "        0.70643997,  0.35979   , -0.48139   , -0.64253998,  0.21908   ,\n",
              "       -0.27849001, -0.22995999, -0.032314  , -0.12598   , -0.28935   ,\n",
              "        0.059438  ,  0.36079001, -0.23272   ,  0.16929001,  0.091287  ,\n",
              "       -0.44764999,  0.30562001, -0.10323   ,  0.47902   , -0.031699  ,\n",
              "       -0.1009    ,  0.20048   ,  0.83442998,  0.57710999,  0.66645002,\n",
              "       -0.41485   , -0.37652001,  0.25514999, -0.24992999, -0.50915998,\n",
              "       -0.35992   , -0.12351   ,  0.35596001,  0.37491   , -0.05548   ,\n",
              "       -0.60108   ,  0.18746001,  0.13669001,  0.3044    ,  0.19981   ,\n",
              "       -0.006172  , -0.50748003,  0.32308999,  0.26857001,  0.23422   ,\n",
              "        0.099418  , -0.7281    , -0.32411   ,  0.53618997,  0.27300999,\n",
              "        0.0012767 ,  0.084466  , -0.29696   ,  0.086806  , -0.29682001,\n",
              "       -0.075324  , -0.31669   ,  0.19908001,  0.16267   ,  0.19402   ,\n",
              "       -0.48322999,  0.22931001, -0.53293002, -0.64165002,  1.2579    ,\n",
              "       -0.19943   ,  0.0081311 ,  0.41953999,  0.06253   ,  0.13387001,\n",
              "       -0.27090999, -0.25729001, -0.12154   ,  0.22368   ,  0.1156    ,\n",
              "       -0.56753999,  0.32565001, -0.19663   , -0.32797   , -0.061104  ,\n",
              "        0.045471  , -0.36296999, -0.46709001, -0.041339  , -0.10877   ,\n",
              "       -0.31126001,  0.088284  , -0.11884   ,  0.12775999,  0.2278    ,\n",
              "       -0.15277   ,  0.11224   ,  0.2133    , -0.31865001, -0.22328   ,\n",
              "       -0.59697002,  0.31641999, -0.24216001,  0.22971   ,  0.75164002,\n",
              "       -0.19707   ,  0.11764   ,  0.097696  , -0.0091149 ,  0.43522999,\n",
              "       -0.12136   , -0.37164   ,  0.019332  , -0.45868   , -0.60272998,\n",
              "       -0.17852999,  0.33972001,  0.03559   ,  0.48407   ,  0.26058999,\n",
              "        0.25753   , -0.25154999, -0.037034  , -0.17206   ,  0.55895001,\n",
              "        0.02566   ,  0.34896001,  0.058973  , -0.58016998, -0.43430001,\n",
              "        0.076113  , -0.065514  ,  0.43768999, -0.89503998, -0.43482   ,\n",
              "       -0.053899  , -0.35696   , -0.14057   , -0.069972  , -0.039529  ,\n",
              "        0.12328   , -0.19384   ,  0.50265002,  0.20554   ,  0.040245  ,\n",
              "        0.022202  ,  0.017232  ,  0.38332999,  0.10679   ,  0.26583001,\n",
              "       -0.24022999,  0.0096255 , -0.11114   ,  0.099512  , -0.03949   ,\n",
              "        0.25139001, -0.30199999,  0.37564999,  0.12596001, -0.11147   ,\n",
              "       -0.36769   ,  0.083935  , -0.02848   , -0.44128001,  0.26440999,\n",
              "       -0.36115   , -0.38949001,  0.07962   , -0.62943   ,  0.36827001,\n",
              "        0.0066196 ,  0.17067   ,  0.16759001,  0.63235998,  0.22036999,\n",
              "       -0.2467    , -0.20492999, -0.55612999,  0.013659  , -0.35642001])"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "embedding_matrix = create_embedding_matrix(embedding_index, word2idx, vocab_size, embedding_dim)\n",
        "embedding_matrix[word2idx[\"statistic\"]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "gsfT7Ie83kvv"
      },
      "source": [
        "Finally, you can create a new model and load the pre-trained word embedding matrix into the `Embedding` layer.\n",
        "\n",
        "You must complete the code for the `create_model_with_embeddings` function. This function takes as input the size of the vocabulary, the number of labels, the `maxlen`, `embedding_dim` and `rnn_units` hyperparameters, and the embedding matrix created by `create_embedding_matrix`. The function should construct, compile and return a RNN equal to the one built in `create_model` with the only difference that the `Embedding` layer must be initialized with the embedding matrix. Use the [Constant](https://keras.io/api/layers/initializers/) initializer for this purpose. For this task, the Embedding layer must be kept trainable so the embeddings can be updated during training.\n",
        "\n",
        "The configuration of the `Embedding` layer for this version of the RNN should look like:\n",
        "> <pre>\n",
        "> {'name': 'embedding_1',\n",
        ">  'trainable': True,\n",
        ">  'batch_input_shape': (None, 130),\n",
        ">  'dtype': 'float32',\n",
        ">  'input_dim': 5508,\n",
        ">  'output_dim': 300,\n",
        ">  'embeddings_initializer': {'class_name': 'Constant',\n",
        ">   'config': {'value': array([[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
        ">             0.        ,  0.        ],\n",
        ">           [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
        ">             0.        ,  0.        ],\n",
        ">           [ 0.72004002,  0.80954999,  0.77170002, ...,  0.39351001,\n",
        ">            -0.47082999, -0.60759002],\n",
        ">           ...,\n",
        ">           [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
        ">             0.        ,  0.        ],\n",
        ">           [-0.40123999, -0.27991   , -0.42445999, ...,  0.45576   ,\n",
        ">             0.61864001, -0.30489001],\n",
        ">           [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
        ">             0.        ,  0.        ]])}},\n",
        ">  'embeddings_regularizer': None,\n",
        ">  'activity_regularizer': None,\n",
        ">  'embeddings_constraint': None,\n",
        ">  'mask_zero': True,\n",
        ">  'input_length': 130}\n",
        "> </pre>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **test_create_model_with_embeddings= 2 Marks**"
      ],
      "metadata": {
        "id": "_TZ8gQY4Ob3J"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "tags": [
          "create_model_with_embeddings"
        ],
        "id": "QsKN22gB3kvv"
      },
      "outputs": [],
      "source": [
        "def create_model_with_embeddings(vocab_size, label_size, maxlen, embedding_dim, rnn_units, embedding_matrix):\n",
        "    #\n",
        "    #  REPLACE THE pass STATEMENT WITH YOUR CODE\n",
        "    #\n",
        "    # Create a Sequential model\n",
        "    model = Sequential()\n",
        "\n",
        "    # Add an Embedding layer with GloVe embeddings\n",
        "    model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=maxlen, mask_zero=True,\n",
        "                        embeddings_initializer=Constant(embedding_matrix)))\n",
        "\n",
        "    # Add a Bidirectional LSTM layer\n",
        "    model.add(Bidirectional(LSTM(units=rnn_units, return_sequences=True)))\n",
        "\n",
        "    # Add a TimeDistributed Dense layer for sequence labeling\n",
        "    model.add(TimeDistributed(Dense(units=label_size, activation='softmax')))\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['sparse_categorical_accuracy'])\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "SMvm0ZxV3kvw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2044919d-831c-424d-d126-c3b926fc1477"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'name': 'embedding_1',\n",
              " 'trainable': True,\n",
              " 'batch_input_shape': (None, 130),\n",
              " 'dtype': 'float32',\n",
              " 'input_dim': 5508,\n",
              " 'output_dim': 300,\n",
              " 'embeddings_initializer': {'class_name': 'Constant',\n",
              "  'config': {'value': array([[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
              "            0.        ,  0.        ],\n",
              "          [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
              "            0.        ,  0.        ],\n",
              "          [ 0.72004002,  0.80954999,  0.77170002, ...,  0.39351001,\n",
              "           -0.47082999, -0.60759002],\n",
              "          ...,\n",
              "          [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
              "            0.        ,  0.        ],\n",
              "          [-0.40123999, -0.27991   , -0.42445999, ...,  0.45576   ,\n",
              "            0.61864001, -0.30489001],\n",
              "          [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
              "            0.        ,  0.        ]])}},\n",
              " 'embeddings_regularizer': None,\n",
              " 'activity_regularizer': None,\n",
              " 'embeddings_constraint': None,\n",
              " 'mask_zero': True,\n",
              " 'input_length': 130}"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "model_with_embeddings = create_model_with_embeddings(vocab_size, label_size, maxlen, embedding_dim, rnn_units, embedding_matrix)\n",
        "model_with_embeddings.get_layer(index=0).get_config()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "LHnOFGa53kvw"
      },
      "source": [
        "Initializing the `Embedding` layer with **GloVe** embeddings should have a positive impact on the model performance for *Quantity* recognition by improving the recall.\n",
        "\n",
        "> <pre>\n",
        ">               precision    recall  f1-score   support\n",
        ">\n",
        ">     Quantity       0.85      0.79      0.82      1263\n",
        ">\n",
        ">    micro avg       0.85      0.79      0.82      1263\n",
        ">    macro avg       0.85      0.79      0.82      1263\n",
        "> weighted avg       0.85      0.79      0.82      1263\n",
        "> </pre>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "ZsrQ8r8a3kvw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83d5d9a4-9386-4521-e7a0-f40e12c2166e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/6\n",
            "22/22 [==============================] - 75s 3s/step - loss: 0.0725 - sparse_categorical_accuracy: 0.9018 - val_loss: 0.0427 - val_sparse_categorical_accuracy: 0.9472\n",
            "Epoch 2/6\n",
            "22/22 [==============================] - 63s 3s/step - loss: 0.0318 - sparse_categorical_accuracy: 0.9539 - val_loss: 0.0319 - val_sparse_categorical_accuracy: 0.9565\n",
            "Epoch 3/6\n",
            "22/22 [==============================] - 67s 3s/step - loss: 0.0222 - sparse_categorical_accuracy: 0.9668 - val_loss: 0.0249 - val_sparse_categorical_accuracy: 0.9641\n",
            "Epoch 4/6\n",
            "22/22 [==============================] - 60s 3s/step - loss: 0.0158 - sparse_categorical_accuracy: 0.9767 - val_loss: 0.0210 - val_sparse_categorical_accuracy: 0.9701\n",
            "Epoch 5/6\n",
            "22/22 [==============================] - 60s 3s/step - loss: 0.0123 - sparse_categorical_accuracy: 0.9819 - val_loss: 0.0193 - val_sparse_categorical_accuracy: 0.9721\n",
            "Epoch 6/6\n",
            "22/22 [==============================] - 63s 3s/step - loss: 0.0096 - sparse_categorical_accuracy: 0.9856 - val_loss: 0.0180 - val_sparse_categorical_accuracy: 0.9750\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x78824e8be470>"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "train_model(model_with_embeddings, x_train, y_train, x_dev, y_dev, batch_size, epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "7B-NyEcl3kvx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3163749-8386-46f3-9444-01ef7a5d5970"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "14/14 [==============================] - 15s 885ms/step\n"
          ]
        }
      ],
      "source": [
        "predictions_with_embeddings = make_predictions(model_with_embeddings, x_test, batch_size)\n",
        "test_data['prediction'] = predictions_to_labels(predictions_with_embeddings, x_test, labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "PWIvdfJq3kvx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "199e0075-80b6-4265-91d8-97cd39e2c9f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Quantity       0.85      0.79      0.82      1263\n",
            "\n",
            "   micro avg       0.85      0.79      0.82      1263\n",
            "   macro avg       0.85      0.79      0.82      1263\n",
            "weighted avg       0.85      0.79      0.82      1263\n",
            "\n"
          ]
        }
      ],
      "source": [
        "evaluate(test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "M9jT-0-63kvx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75561abd-939f-4e3e-a1d8-b18340a144e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.10/dist-packages (6.5.4)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from nbconvert) (4.9.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from nbconvert) (4.11.2)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from nbconvert) (6.0.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.10/dist-packages (from nbconvert) (0.7.1)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from nbconvert) (0.4)\n",
            "Requirement already satisfied: jinja2>=3.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert) (3.1.2)\n",
            "Requirement already satisfied: jupyter-core>=4.7 in /usr/local/lib/python3.10/dist-packages (from nbconvert) (5.3.1)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.10/dist-packages (from nbconvert) (0.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert) (2.1.3)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert) (0.8.4)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert) (0.8.0)\n",
            "Requirement already satisfied: nbformat>=5.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert) (5.9.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from nbconvert) (23.1)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert) (1.5.0)\n",
            "Requirement already satisfied: pygments>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert) (2.16.1)\n",
            "Requirement already satisfied: tinycss2 in /usr/local/lib/python3.10/dist-packages (from nbconvert) (1.2.1)\n",
            "Requirement already satisfied: traitlets>=5.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert) (5.7.1)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core>=4.7->nbconvert) (3.10.0)\n",
            "Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.10/dist-packages (from nbclient>=0.5.0->nbconvert) (6.1.12)\n",
            "Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.10/dist-packages (from nbformat>=5.1->nbconvert) (2.18.0)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.10/dist-packages (from nbformat>=5.1->nbconvert) (4.19.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->nbconvert) (2.5)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from bleach->nbconvert) (1.16.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->nbconvert) (0.5.1)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.1->nbconvert) (23.1.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.1->nbconvert) (2023.7.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.1->nbconvert) (0.30.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.1->nbconvert) (0.10.2)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.10/dist-packages (from jupyter-client>=6.1.12->nbclient>=0.5.0->nbconvert) (23.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.10/dist-packages (from jupyter-client>=6.1.12->nbclient>=0.5.0->nbconvert) (2.8.2)\n",
            "Requirement already satisfied: tornado>=4.1 in /usr/local/lib/python3.10/dist-packages (from jupyter-client>=6.1.12->nbclient>=0.5.0->nbconvert) (6.3.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install nbconvert"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!jupyter nbconvert --to html /content/Assignment_3_Learner_Template_sequencelabeling.ipynb"
      ],
      "metadata": {
        "id": "3KIsRaW1ykPg"
      },
      "execution_count": 35,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}